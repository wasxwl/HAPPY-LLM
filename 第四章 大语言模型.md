4.1 什么是LLM

    1.LLM的定义
        LLM，即 Large Language Model，中文名为大语言模型或大型语言模型，是一种相较传统语言模型参数量更多、在更大规模语料上进行预训练的语言模型。
        一般来说，LLM 指包含数百亿（或更多）参数的语言模型，它们具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，广义的 LLM 一般覆盖了从十亿参数（如 Qwen-1.5B）到千亿参数（如 Grok-314B）的所有大型语言模型。只要模型展现出涌现能力，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。
        一般认为，GPT-3（1750亿参数）是 LLM 的开端，基于 GPT-3 通过 预训练、监督微调、强化学习与人类反馈三阶段训练得到的 ChatGPT 更是主导了 LLM 时代的到来。

    2.LLM的能力
        1.涌现能力（区分 LLM 与传统 PLM 最显著的特征）
            涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。
            量变引起质变。
        2.上下文学习
            上下文学习是指允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。
        3.指令遵循
            指通过使用自然语言描述的多任务数据进行微调。
            指令遵循能力意味我们不再需要每一件事都先教模型，然后它才能去做。我们只需要在指令微调阶段混合多种指令来训练其泛化能力，LLM 可以灵活地解决用户遇到的问题。
        4.逐步推理
            逐步推理能力意味着 LLM 可以处理复杂逻辑任务，也就是说可以解决日常生活中需要逻辑判断的绝大部分问题，从而向“可靠的”智能助理迈出了坚实的一步。

    3.LLM的特点
        1.多语言支持
            训练语料是多语言的，所以LLM 天生即具有多语言、跨语言能力，只不过在不同语言上的能力有所差异。
            由于英文高质量语料目前仍是占据大部分，以 GPT-4 为代表的绝大部分模型在英文上具有显著超越中文的能力。针对中文进行额外训练和优化的国内模型（如文心一言、通义千问等）往往能够在中文环境上展现更优越的效果。
        2.长文本处理
            LLM 往往比传统 PLM 更看重长文本处理能力。
        3.拓展多模态
            LLM 的强大能力也为其带来了跨模态的强大表现。
        4.挥之不去的幻觉
            幻觉，是指 LLM 根据 Prompt 杜撰生成虚假、错误信息的表现。
            幻觉问题是 LLM 的固有缺陷，也是目前 LLM 研究及应用的巨大挑战。
            目前有很多研究提供了削弱幻觉的一些方法，如 Prompt 里进行限制、通过 RAG（检索增强生成）来指导生成等，但都还只能减弱幻觉而无法彻底根除。

4.2 如何训练一个LLM

一般而言，训练一个完整的 LLM 需要经过三个阶段——Pretrain、SFT 和 RLHF。

    1.Pretrain
        Pretrain，即预训练，是训练 LLM 最核心也是工程量最大的第一步。
        庞大的模型参数和预训练数据使得分布式训练框架成为 LLM 训练必不可少的组成部分。
        训练一个中文 LLM，训练数据的难度会更大。目前，高质量语料还是大部分集中在英文范畴。目前开源的中文预训练数据集仅有昆仑天工开源的SkyPile（150B）、中科闻歌开源的yayi2（100B）等，相较于英文开源数据集有明显差距。

        预训练数据的处理与清洗也是 LLM 预训练的一个重要环节，预训练数据的质量往往比体量更加重要。预训练数据处理一般包括以下流程：

            1.文档准备。由于海量预训练语料往往是从互联网上获得，一般需要从爬取的网站来获得自然语言文档。文档准备主要包括 URL 过滤（根据网页 URL 过滤掉有害内容）、文档提取（从 HTML 中提取纯文本）、语言选择（确定提取的文本的语种）等。
            2.语料过滤。语料过滤的核心目的是去除低质量、无意义、有毒有害的内容，例如乱码、广告等。
            语料过滤一般有两种方法：基于模型的方法，即通过高质量语料库训练一个文本分类器进行过滤；基于启发式的方法，一般通过人工定义 web 内容的质量指标，计算语料的指标值来进行过滤。
            3.语料去重。实验表示，大量重复文本会显著影响模型的泛化能力。去重一般基于 hash 算法计算数据集内部或跨数据集的文档相似性，将相似性大于指定阈值的文档去除；也可以基于子串在序列级进行精确匹配去重。

        目前，已有很多经过处理的高质量预训练语料和专用于预训练数据处理的框架。

    2.SFT
        我们需要第二步来教这个博览群书的学生如何去使用它的知识，也就是 SFT（Supervised Fine-Tuning，有监督微调）。所谓有监督微调，其实就是对于能力有限的传统预训练模型，我们需要针对每一个下游任务单独对其进行微调以训练模型在该任务上的表现。例如要解决文本分类问题，需要对 BERT 进行文本分类的微调；要解决实体识别的问题，就需要进行实体识别任务的微调。

        所谓指令微调，即我们训练的输入是各种类型的用户指令，而需要模型拟合的输出则是我们希望模型在收到该指令后做出的回复。例如，我们的一条训练样本可以是：
            input:告诉我今天的天气预报？
            output:根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦

        SFT 的主要目标是让模型从各种指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。因此，类似于 Pretrain，SFT 的数据质量和数据配比也是决定模型指令遵循能力的重要因素。
        首先是指令数据量及覆盖范围。为了使 LLM 能够获得泛化的指令遵循能力，即能够在未训练的指令上表现良好，需要收集大量类别各异的用户指令和对应回复对 LLM 进行训练。

        模型是否支持多轮对话，与预训练是没有关系的。事实上，模型的多轮对话能力完全来自于 SFT 阶段。如果要使模型支持多轮对话，我们需要在 SFT 时将训练数据构造成多轮对话格式，让模型能够利用之前的知识来生成回答。

        构造多轮对话样本一般有三种方式：
            1.直接将最后一次模型回复作为输出，前面所有历史对话作为输入，直接拟合最后一次回复
            2.将 N 轮对话构造成 N 个样本
            3.直接要求模型预测每一轮对话的输出
        显然可知，第一种方式会丢失大量中间信息，第二种方式造成了大量重复计算，只有第三种方式是最合理的多轮对话构造。

    3.RLHF
        RLHF，全称是 Reinforcement Learning from Human Feedback，即人类反馈强化学习，是利用强化学习来训练 LLM 的关键步骤。
        RLHF 就类似于 LLM 作为一个学生，不断做作业来去提升自己解题能力的过程。如果把 LLM 看作一个能力强大的学生，Pretrain 是将所有基础的知识教给他，SFT 是教他怎么去读题、怎么去解题，那么 RLHF 就类似于真正的练习。LLM 会不断根据 Pretrain 学到的基础知识和 SFT 学到的解题能力去解答练习，然后人类作为老师批改 LLM 的练习，来让 LLM 反思错误的解题方式，不断强化正确的解题方式。

        RLHF 分为两个步骤：训练 RM 和 PPO 训练。
            1.RM，Reward Model，即奖励模型。RM 是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM 的每一个回复，RM 会进行打分，打分反映了生成回复符合人类偏好的程度。然后 LLM 会根据强化学习的原理，基于 RM 的打分来进行优化训练。
            2.在完成 RM 训练之后，就可以使用 PPO 算法来进行强化学习训练。PPO，Proximal Policy Optimization，近端策略优化算法，是一种经典的 RL 算法。
            在具体 PPO 训练过程中，会存在四个模型：两个 LLM 和两个 RM。两个 LLM 分别是进行微调、参数更新的 actor model 和不进行参数更新的 ref model，均是从 SFT 之后的 LLM 初始化的。两个 RM 分别是进行参数更新的 critic model 和不进行参数更新的 reward model，均是从上一步训练的 RM 初始化的。