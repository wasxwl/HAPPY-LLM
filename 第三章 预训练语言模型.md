三种核心的模型架构:Encoder-Only、Encoder-Decoder、Decoder-Only

3.1 Encoder-only PLM

    1.BERT:一个统一了多种思想的预训练模型
        1.核心思想包括Transformer 架构和预训练+微调范式
        2.模型架构：Encoder-only
            BERT 采用 WordPiece 作为分词方法。WordPiece 是一种基于统计的子词切分算法，其核心在于将单词拆解为子词（例如，"playing" -> ["play", "##ing"]）。其合并操作的依据是最大化语言模型的似然度。对于中文等非空格分隔的语言，通常将单个汉字作为原子分词单位（token）处理。
        3.预训练任务--MLM+NSP
            BERT 创新点：两个新的预训练任务上——MLM 和 NSP
            MLM 模拟的是“完形填空”。MLM 的思路也很简单，在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token。
            NSP，即下一个句子预测。NSP 的核心思想是针对句级的 NLU 任务，例如问答匹配、自然语言推理等。NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文。
        4.下游任务微调
            BERT 的重大意义就是正式确立了预训练-微调的两阶段思想。该思想重点在于，预训练得到的强大能力能否通过低成本的微调快速迁移到对应的下游任务上。

    2.RoBERTa
        1.优化一：去掉NSP预训练任务
            RoBERTa 在预训练中去掉了 NSP，只使用 MLM 任务。
            RoBERTa 对 MLM 任务本身也做出了改进：后续 MLM 任务基本都使用了动态遮蔽。
        2.优化二：更大规模的预训练数据和预训练步长
            RoBERTa 使用了更大量的无监督语料进行预训练，RoBERTa 认为更大的 batch size 既可以提高优化速度，也可以提高任务结束性能。
        3.优化三：更大的bpe词表
            RoBERTa 使用了 BPE 作为 Tokenizer 的编码策略。BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。
            一般来说，BPE 编码的词典越大，编码效果越好。
    
    3.ALBERT
    ALBERT 成功地以更小规模的参数实现了超越 BERT 的能力。
        1.优化一：将Embedding 参数进行分解
            ALBERT 对 Embedding 层的参数矩阵进行了分解，让 Embedding 层的输出维度和隐藏层维度解绑，也就是在 Embedding 层的后面加入一个线性矩阵进行维度变换。
        2.优化二：夸层进行参数共享
            ALBERT 发现各个 Encoder 层的参数出现高度一致的情况。因此，ALBERT 提出，可以让各个 Encoder 层共享模型参数，来减少模型的参数量。
            在具体实现上，其实就是 ALBERT 仅初始化了一个 Encoder 层。
        3.提出SOP预训练任务
            ALBERT 认为 NSP 任务过于简单，在预训练中无法对模型效果的提升带来显著影响。但是不同于 RoBERTa 选择直接去掉 NSP，ALBERT 选择改进 NSP，增加其难度，来优化模型的预训练。
            SOP 预训练任务对模型效果有显著提升。使用 MLM + SOP 预训练的模型效果优于仅使用 MLM 预训练的模型更优于使用 MLM + NSP 预训练的模型。

3.2 Encoder-Decoder PLM

BERT 通过预训练任务 MLM 和 NSP 来学习文本的双向语义关系，从而在下游任务中取得了优异的性能。
但是，BERT 也存在一些问题，例如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等问题。
研究者们提出了 Encoder-Decoder 模型，通过引入 Decoder 部分来解决这些问题。

    1.T5（Text-To-Text Transfer Transformer）
        T5 基于 Transformer 架构，包含编码器和解码器两个部分。
        1.模型结构：Encoder-Decoder
            编码器用于处理输入文本，解码器用于生成输出文本。
        2.预训练任务
            T5 模型的预训练任务是一个关键的组成部分，它能使模型能够学习到丰富的语言表示，语言表示能力可以在后续的微调过程中被迁移到各种下游任务。
            T5 的预训练任务，主要包括以下几个部分：
                1.预训练任务: T5模型的预训练任务是 MLM，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程可以在大量未标注的文本上进行。
                2.输入格式: 预训练时，T5将输入文本转换为"文本到文本"的格式。
                3.预训练数据集: T5 使用了自己创建的大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集经过了一定的清洗，去除了无意义的文本、重复文本等。
                4.多任务预训练: T5 还尝试了将多个任务混合在一起进行预训练。这有助于模型学习更通用的语言表示。
                5.预训练到微调的转换: 预训练完成后，T5模型会在下游任务上进行微调。微调时，模型在任务特定的数据集上进行训练，并根据任务调整解码策略。
        3.大一统思想：所有的 NLP 任务都可以统一为文本到文本的任务
            T5通过大规模的文本数据进行预训练，然后在具体任务上进行微调。T5的大一统思想简化了任务处理流程，增强了模型的通用性和适应性。

3.3 Decoder-Only PLM

目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）。

    1.GPT
        1.模型架构--Decoder-Only
            GPT 的整体结构和 BERT 是有一些类似的，只是相较于 BERT 的 Encoder，选择使用了 Decoder 来进行模型结构的堆叠。
        2.预训练任务--CLM
            Decoder-Only 模型往往选择最传统直接的预训练任务——因果语言模型，Casual Language Model，简称 CLM。
            CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，通过不断重复该过程来实现目标文本序列的生成。
            CLM 是一个经典的补全形式。
            CLM 的输入和输出可以是：
                input: 今天天气
                output: 今天天气很
                input: 今天天气很
                output：今天天气很好
            CLM 是更直接的预训练任务，其天生和人类书写自然语言文本的习惯相契合，也和下游任务直接匹配，相对于 MLM 任务更加直接，可以在任何自然语言文本上直接应用。因此，CLM 也可以使用海量的自然语言语料进行大规模的预训练。
        3.GPT系列模型的发展
            模型	Decoder Layer	Hidden_size	   注意力头数	  注意力维度	总参数量	预训练语料
            GPT-1	    12	            3072	    12	            768	        0.12B	    5GB
            GPT-2	    48	            6400	    25	            1600	    1.5B	    40GB
            GPT-3	    96	            49152	    96	            12288	    175B	    570GB

    2.LLaMA
    LLaMA模型是由Meta（前Facebook）开发的一系列大型预训练语言模型。
        1.模型架构--Decoder-Only
            与GPT系列模型一样，LLaMA模型也是基于Decoder-Only架构的预训练语言模型,处理流程也始于将输入文本通过tokenizer进行编码，转化为一系列的input_ids。
        2.LLaMA模型的发展历程
            LLaMA模型以其技术创新、多参数版本、大规模预训练和高效架构设计而著称。
            LLaMA-1以其开源性和优异性能迅速受到社区欢迎，而LLaMA-2和LLaMA-3进一步通过引入分组查询注意力机制和支持更长文本输入，显著提升了模型性能和应用范围。特别是LLaMA-3，通过采用128K词表大小的高效tokenizer和15T token的庞大训练数据，实现了在多语言和多任务处理上的重大进步。

    3.GLM
        GLM 系列模型是由智谱开发的主流中文 LLM 之一。
        1.模型架构--相对于GPT的略微修正
            GLM 最初是由清华计算机系推出的一种通用语言模型基座，其核心思路是在传统 CLM 预训练任务基础上，加入 MLM 思想，从而构建一个在 NLG 和 NLU 任务上都具有良好表现的统一模型。
            GLM 和 GPT 大致类似，均是 Decoder-Only 的结构，仅有三点细微差异：
                1.使用 Post Norm 而非 Pre Norm。
                2.使用单个线性层实现最终 token 的预测，而不是使用 MLP。
                3.激活函数从 ReLU 换成了 GeLUS。
        2.预训练任务GLM
            GLM 是一种结合了自编码思想和自回归思想的预训练方法。所谓自编码思想，其实也就是 MLM 的任务学习思路，在输入文本中随机删除连续的 tokens，要求模型学习被删除的 tokens；所谓自回归思想，其实就是传统的 CLM 任务学习思路，也就是要求模型按顺序重建连续 tokens。